{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load matcher\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5d9bc5d55d4acba64b1e43299e0b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d45348838b646138eb0339d1da1ecfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92aea10ebee435aa6ac5d3287b66b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import skimage.transform\n",
    "import PIL.Image as pil\n",
    "import tqdm\n",
    "import os\n",
    "os.sys.path.append(\"/home/data/workspace/heqi/monogastroendo\")\n",
    "from utils import *\n",
    "\n",
    "fpath = os.path.join(\"/home/data/workspace/heqi/matchingloss/splits/simcol_complete\", \"{}_files.txt\")\n",
    "data_path = \"/home/data/workspace/heqi/matchingloss/data/simcol_complete/imgs\"\n",
    "img_ext = \".png\"\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning\n",
    "    # (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "        \n",
    "def get_image_path(folder, frame_index_str):\n",
    "    f_str = \"{}{}\".format(frame_index_str, img_ext)\n",
    "    image_path = os.path.join(data_path, folder, f_str)\n",
    "    return image_path\n",
    "\n",
    "def get_color(folder, frame_index_str, do_flip):\n",
    "    color = pil_loader(get_image_path(folder, frame_index_str))\n",
    "    \n",
    "    if do_flip:\n",
    "        color = color.transpose(pil.FLIP_LEFT_RIGHT)\n",
    "    return color\n",
    "\n",
    "# utils\n",
    "resize = torchvision.transforms.Resize((352, 352), interpolation=torchvision.transforms.InterpolationMode.LANCZOS)\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "torch.cuda.set_device(1)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class LoFTR(nn.Module):\n",
    "    \"\"\"Layer to compute the correspondences between a pair of images\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained='indoor'):\n",
    "        super(LoFTR, self).__init__()\n",
    "        self.matcher = KF.LoFTR(pretrained=pretrained)\n",
    "\n",
    "    def forward(self, src0, srcx):\n",
    "        input_dict = {\"image0\": K.color.rgb_to_grayscale(src0), # LofTR works on grayscale images only \n",
    "                    \"image1\": K.color.rgb_to_grayscale(srcx)}\n",
    "        with torch.no_grad():\n",
    "            correspondences = self.matcher(input_dict)\n",
    "        return correspondences\n",
    "##########################################################\n",
    "\n",
    "# load data\n",
    "train_filenames = readlines(fpath.format(\"train\"))\n",
    "val_filenames = readlines(fpath.format(\"val\"))\n",
    "test_filenames = readlines(fpath.format(\"test\"))\n",
    "\n",
    "# load matcher\n",
    "try:\n",
    "    matcher\n",
    "    print(\"matcher loaded\")\n",
    "except NameError:\n",
    "    print(\"load matcher\")\n",
    "    matcher = LoFTR(pretrained=\"indoor\")\n",
    "    matcher.to(device)\n",
    "\n",
    "# processing correspondence\n",
    "matcher_result = {\"no_flip\": [],\n",
    "                  \"do_flip\": []}\n",
    "for i in tqdm.notebook.tnrange(len(val_filenames)):\n",
    "    line = val_filenames[i].split()\n",
    "    \n",
    "    for do_flip in [False, True]:\n",
    "        img_tensor = []\n",
    "        for j in range(3):\n",
    "            img_tensor.append(to_tensor(resize(get_color(line[0], line[1+j], do_flip))).to(device))\n",
    "        correspondences = []\n",
    "        correspondences.append(matcher.forward(img_tensor[1][None, ...], img_tensor[0][None, ...]))\n",
    "        correspondences.append(matcher.forward(img_tensor[1][None, ...], img_tensor[2][None, ...]))\n",
    "        for k in range(2):\n",
    "            del correspondences[k]['batch_indexes']\n",
    "            correspondences[k]['keypoints0'] = correspondences[k]['keypoints0'].detach().cpu().numpy()\n",
    "            correspondences[k]['keypoints1'] = correspondences[k]['keypoints1'].detach().cpu().numpy()\n",
    "            correspondences[k]['confidence'] = correspondences[k]['confidence'].detach().cpu().numpy()\n",
    "        if do_flip:\n",
    "            matcher_result[\"do_flip\"].append(correspondences)\n",
    "        else:\n",
    "            matcher_result[\"no_flip\"].append(correspondences)\n",
    "    \n",
    "np.save(\"val_352x352\", matcher_result)\n",
    "\n",
    "# processing correspondence\n",
    "matcher_result = {\"no_flip\": [],\n",
    "                  \"do_flip\": []}\n",
    "for i in tqdm.notebook.tnrange(len(train_filenames)):\n",
    "    line = train_filenames[i].split()\n",
    "    \n",
    "    for do_flip in [False, True]:\n",
    "        img_tensor = []\n",
    "        for j in range(3):\n",
    "            img_tensor.append(to_tensor(resize(get_color(line[0], line[1+j], do_flip))).to(device))\n",
    "        correspondences = []\n",
    "        correspondences.append(matcher.forward(img_tensor[1][None, ...], img_tensor[0][None, ...]))\n",
    "        correspondences.append(matcher.forward(img_tensor[1][None, ...], img_tensor[2][None, ...]))\n",
    "        for k in range(2):\n",
    "            del correspondences[k]['batch_indexes']\n",
    "            correspondences[k]['keypoints0'] = correspondences[k]['keypoints0'].detach().cpu().numpy()\n",
    "            correspondences[k]['keypoints1'] = correspondences[k]['keypoints1'].detach().cpu().numpy()\n",
    "            correspondences[k]['confidence'] = correspondences[k]['confidence'].detach().cpu().numpy()\n",
    "        if do_flip:\n",
    "            matcher_result[\"do_flip\"].append(correspondences)\n",
    "        else:\n",
    "            matcher_result[\"no_flip\"].append(correspondences)\n",
    "    \n",
    "np.save(\"train_352x352\", matcher_result)\n",
    "\n",
    "# processing correspondence\n",
    "matcher_result = {\"no_flip\": [],\n",
    "                  \"do_flip\": []}\n",
    "for i in tqdm.notebook.tnrange(len(test_filenames)):\n",
    "    line = test_filenames[i].split()\n",
    "    \n",
    "    for do_flip in [False, True]:\n",
    "        img_tensor = []\n",
    "        for j in range(3):\n",
    "            img_tensor.append(to_tensor(resize(get_color(line[0], line[1+j], do_flip))).to(device))\n",
    "        correspondences = []\n",
    "        correspondences.append(matcher.forward(img_tensor[1][None, ...], img_tensor[0][None, ...]))\n",
    "        correspondences.append(matcher.forward(img_tensor[1][None, ...], img_tensor[2][None, ...]))\n",
    "        for k in range(2):\n",
    "            del correspondences[k]['batch_indexes']\n",
    "            correspondences[k]['keypoints0'] = correspondences[k]['keypoints0'].detach().cpu().numpy()\n",
    "            correspondences[k]['keypoints1'] = correspondences[k]['keypoints1'].detach().cpu().numpy()\n",
    "            correspondences[k]['confidence'] = correspondences[k]['confidence'].detach().cpu().numpy()\n",
    "        if do_flip:\n",
    "            matcher_result[\"do_flip\"].append(correspondences)\n",
    "        else:\n",
    "            matcher_result[\"no_flip\"].append(correspondences)\n",
    "    \n",
    "np.save(\"test_352x352\", matcher_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16.,  16.],\n",
       "       [ 40.,  16.],\n",
       "       [ 48.,  16.],\n",
       "       ...,\n",
       "       [368., 424.],\n",
       "       [408., 424.],\n",
       "       [416., 424.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher_result['no_flip'][0][0][\"keypoints0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del matcher_result_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 72.,  16.],\n",
       "       [ 80.,  16.],\n",
       "       [ 88.,  16.],\n",
       "       ...,\n",
       "       [400., 424.],\n",
       "       [408., 424.],\n",
       "       [424., 424.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    matcher_result_load\n",
    "except NameError:\n",
    "    matcher_result_load = np.load(\"matcher_result.npy\", allow_pickle=True).all()\n",
    "matcher_result_load['do_flip'][0][0][\"keypoints0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'keypoints0': array([[ 72.,  16.],\n",
       "         [ 80.,  16.],\n",
       "         [ 88.,  16.],\n",
       "         ...,\n",
       "         [400., 424.],\n",
       "         [408., 424.],\n",
       "         [424., 424.]], dtype=float32),\n",
       "  'keypoints1': array([[ 40.52599 ,  56.303223],\n",
       "         [ 48.312782,  56.0391  ],\n",
       "         [ 55.917706,  55.54209 ],\n",
       "         ...,\n",
       "         [375.33487 , 376.00107 ],\n",
       "         [382.4657  , 375.67676 ],\n",
       "         [392.83636 , 375.35797 ]], dtype=float32),\n",
       "  'confidence': array([0.663439  , 0.99974006, 0.8829035 , ..., 1.        , 0.9891658 ,\n",
       "         0.9999306 ], dtype=float32)},\n",
       " {'keypoints0': array([[112.,  16.],\n",
       "         [128.,  16.],\n",
       "         [136.,  16.],\n",
       "         ...,\n",
       "         [ 88., 424.],\n",
       "         [ 96., 424.],\n",
       "         [104., 424.]], dtype=float32),\n",
       "  'keypoints1': array([[119.538445,  16.027597],\n",
       "         [128.78128 ,  16.262442],\n",
       "         [136.65668 ,  17.015198],\n",
       "         ...,\n",
       "         [ 56.229065, 414.63205 ],\n",
       "         [ 64.1382  , 414.94138 ],\n",
       "         [ 72.83633 , 414.48016 ]], dtype=float32),\n",
       "  'confidence': array([0.980453  , 0.33656985, 0.9999455 , ..., 0.99985754, 0.9999778 ,\n",
       "         0.9888664 ], dtype=float32)}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher_result_load['do_flip'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
